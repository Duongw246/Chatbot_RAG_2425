import time
import random
import streamlit as st
from seed_data import postgres_retriever
from agent import (legal_response, 
                   normal_response,
                   compare_legal, 
                   query_transform,
                   get_router
                   )
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

def reset_conversation():
    """
    ƒê·∫∑t l·∫°i to√†n b·ªô tr·∫°ng th√°i c·ªßa cu·ªôc tr√≤ chuy·ªán.
    """
    st.session_state['messages'] = [
        {"role": "assistant", "content": random.choice(
            [
                "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω c·ªßa b·∫°n. H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
                "Ch√†o b·∫°n! B·∫°n c·∫ßn t√¥i gi√∫p g√¨ kh√¥ng? T√¥i c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô c·ªßa b·∫°n.",
                "B·∫°n mu·ªën bi·∫øt g√¨ v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô? H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
        ])}
    ] 
    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    msgs.clear()  
    st.rerun()

def response_generator(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.05)
    
def setup_page():
    """
    C·∫•u h√¨nh trang web c∆° b·∫£n
    """
    st.set_page_config(
        page_title="Legal Assistant", 
        page_icon="üí¨",
        layout="wide" 
    )

def setup_sidebar():
    with st.sidebar:
        st.markdown("""
        <h1 style='text-align: center; color: #4CAF50;'>‚öôÔ∏è C·∫•u H√¨nh</h1>
        """, unsafe_allow_html=True)
        
        with st.expander("üöÄ Ch·ªçn Model AI", expanded=True):
            model_choice = st.selectbox(
                "Ch·ªçn Model ƒë·ªÉ tr·∫£ l·ªùi:",
                ["gemini-1.5-pro", "gemini-1.5-flash"],
                index=0
            )
            st.caption("üîπ Model AI s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi.")
            
        with st.expander("üõ† Tu·ª≥ Ch·ªçn Kh√°c"):
            if st.button("üóë X√≥a cu·ªôc tr√≤ chuy·ªán", use_container_width=True):
                reset_conversation()
            st.markdown("""
            <small style='color: grey;'>X√≥a l·ªãch s·ª≠ chat ƒë·ªÉ b·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi.</small>
            """, unsafe_allow_html=True)
        
    return model_choice
        
def setup_chat_interface(model_choice):
    st.title("Legal Assistant")
    if model_choice == "gemini-1.5-pro" or model_choice == "gemini-1.5-flash":
        st.caption("Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† Google")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    if "messages" not in st.session_state:
        st.session_state['messages'] = [
            {"role": "assistant", "content": random.choice(
                [
                    "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω c·ªßa b·∫°n. H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
                    "Ch√†o b·∫°n! B·∫°n c·∫ßn t√¥i gi√∫p g√¨ kh√¥ng? T√¥i c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô c·ªßa b·∫°n.",
                    "B·∫°n mu·ªën bi·∫øt g√¨ v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô? H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
            ])}
        ]
        msgs.add_ai_message(st.session_state.messages[0]["content"])
    
    for msg in st.session_state['messages']:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    return msgs

def user_input(msgs, model_choice, new_retriever, old_retriever):
    if prompt:= st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô!"):
        st.session_state['messages'].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)   
        msgs.add_user_message(prompt)
        router = get_router(prompt, model_choice)
        router, status = router.split(",")
        router = router.strip(" ")
        status = status.strip(" ")
        # st.markdown(router)
        # st.markdown(status)
        transform_prompt = query_transform(prompt, model_choice)
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            timer_placeholder = st.empty()
            start_time = time.time()  
            with st.spinner("ƒêang suy nghƒ©..."):
                def update_timer():
                    while True:
                        elapsed_time = time.time() - start_time
                        timer_placeholder.caption(f"‚è±Ô∏è ƒêang x·ª≠ l√Ω: {elapsed_time:.2f} gi√¢y")
                        time.sleep(0.1)

                import threading
                timer_thread = threading.Thread(target=update_timer, daemon=True)
                timer_thread.start()
                chat_history = [
                    {"role": msg["role"], "content": msg["content"]}
                    for msg in st.session_state.messages[:-1]
                ]
                if router == "yes":
                    if status == "new":
                        st.caption("I'm in YES, NEW")
                        context = new_retriever.invoke(transform_prompt)
                        
                    elif status == "old":
                        st.caption("I'm in YES, OLD")
                        context = old_retriever.invoke(transform_prompt)

                    response = legal_response(transform_prompt, model_choice, context, chat_history)
                        
                elif router == "no":
                    st.caption("I'm in NO")
                    response = normal_response(transform_prompt, model_choice, chat_history)
                
                elif router == "compare":
                    st.caption("I'm in COMPARE")
                    old_context = old_retriever.invoke(transform_prompt)
                    new_context = new_retriever.invoke(transform_prompt)
                    response =  compare_legal(transform_prompt, model_choice, old_context, new_context)
                else:
                    response = random.choice([
                    "C√¢u h·ªèi c·ªßa b·∫°n n·∫±m kh√¥ng n·∫±m trong ph·∫°m vi v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô. H√£y nh·∫≠p c√¢u h·ªèi kh√°c ƒë·ªÉ t√¥i gi√∫p b·∫°n nh√©!",
                    "Xin l·ªói, c√¢u h·ªèi n√†y kh√¥ng n·∫±m trong ph·∫°m vi c·ªßa t√¥i. H√£y nh·∫≠p c√¢u h·ªèi kh√°c ƒë·ªÉ t√¥i gi√∫p b·∫°n nh√©!",
                    "Ki·∫øn th·ª©c c·ªßa t√¥i ch·ªâ gi·ªõi h·∫°n trong lƒ©nh v·ª±c lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô. H√£y nh·∫≠p c√¢u h·ªèi kh√°c ƒë·ªÉ t√¥i gi√∫p b·∫°n nh√©!",
                ])  
                response_content = ""
                for chunk in response:
                    for char in chunk:
                        response_content += char
                        response_placeholder.markdown(response_content)
                end_time = time.time()  # K·∫øt th√∫c ƒëo th·ªùi gian
                elapsed_time = end_time - start_time    
                st.session_state['messages'].append({"role": "assistant", "content": response})
                msgs.add_ai_message(response)
                elapsed_time = time.time() - start_time
                timer_placeholder.caption(f"‚è±Ô∏è Th·ªùi gian ph·∫£n h·ªìi: {elapsed_time:.2f} gi√¢y")
def main():
    setup_page()
    model_choice = setup_sidebar()
    new_retriever = postgres_retriever(collection_name="vectordb", database_name="new_legal")
    old_retriever = postgres_retriever(collection_name="old_vectordb", database_name="old_legal")
    msgs = setup_chat_interface(model_choice)
    user_input(msgs, model_choice, new_retriever, old_retriever)
    
if __name__ == "__main__":
    main()