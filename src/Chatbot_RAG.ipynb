{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TON DUC THANG IT PROJECT (ProjectIT_504091_2425)**\n",
    "### ***- @Author:** 521H0220 Bui Hai Duong*\n",
    "### ***- @Instructor:** Assoc. Prof. PhD. Le Anh Cuong*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Import libraries for RAG***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document as LC_Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.prompts import  PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VnCoreNLP model folder C:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py_vncorenlp already exists! Please load VnCoreNLP from this folder!\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "py_vncorenlp.download_model(save_dir=r\"C:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py_vncorenlp\")\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=r\"C:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py_vncorenlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Environment Variable***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PINECONE_API_KEY'] = \"pcsk_3EcDrL_3mUVa7rhMVLMFBZJFPvgGEaunymPs7T5XcZXjBp9dF55S73miNRzeW2FMsFWcEb\"\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyD6fv2qZAcRc30uDjn96CbsM6pUJwLkdFE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = \"focused-service-447410-q8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCekUE-sNiAc_Jw-TFaLO11Xn18lLc-Lkw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Checking whether GPU is available***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_name = torch.cuda.get_device_name(device=device)\n",
    "device_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Prompting function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_pro() -> GoogleGenerativeAI:\n",
    "    return GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ['GOOGLE_API_KEY'], temperature=0)\n",
    "\n",
    "def get_gemini_flash() -> GoogleGenerativeAI:\n",
    "    return GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=os.environ['GOOGLE_API_KEY'], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_transform(query: str, model_choice) -> str:\n",
    "    system_template = \"\"\"\n",
    "    Bạn là một chuyên gia về việc chuyển đổi câu hỏi.\n",
    "    Nhiệm vụ của bạn là chuyển đổi câu hỏi của người dùng để có thể sử dụng trong việc truy vấn văn bản luật giao thông đường bộ tốt hơn.\n",
    "    Các câu hỏi thông thường sẽ là câu hỏi về luật.\n",
    "    \n",
    "    Đây là câu hỏi từ người dùng:\n",
    "    {query}\n",
    "    \n",
    "    ### Hướng dẫn:\n",
    "    1. **Với những câu hỏi về luật cũ và luật mới**\n",
    "    - Nếu các câu hỏi chỉ rõ rằng đó là luật mới, hoặc luật cũ trong câu hỏi thì hãy bỏ những từ đó đi.\n",
    "    VD: \n",
    "    - \"Luật giao thông mới về việc điều khiển xe máy\" -> \"Luật giao thông về việc điều khiển xe máy\"\n",
    "    - \"Luật cũ về việc chở quá số người quy định\" -> \"Luật về việc chở quá số người quy định\"\n",
    "    \n",
    "    2. **Với những câu hỏi đơn giản về luật hoặc những câu hỏi không phải luật giao thông đường bộ**\n",
    "    - Hãy giữ nguyên câu hỏi và không thay đổi.\n",
    "    VD:\n",
    "    - \"Tốc độ tối đa được phép chạy trong khu dân cư là bao nhiêu?\"\n",
    "    - \"Người đi bộ có được phép băng qua đường tại nơi không có vạch kẻ đường không?\"\n",
    "    - \"Xin chào bạn!\"\n",
    "    - \"Hôm nay thời tiết như thế nào?\"\n",
    "    \n",
    "    3. **Với những câu hỏi phức tạp về luật**\n",
    "    - Nếu câu hỏi có nhiều chi tiết, hãy chuyển đổi câu hỏi thành câu hỏi đơn giản và bao quát hơn.\n",
    "    VD:\n",
    "    - \"Tôi muốn biết về việc điều khiển xe máy trong thời gian gần đây\" -> \"Luật về việc điều khiển xe máy\"\n",
    "    - \"Tôi điều khiển xe máy với vận tốc là 100km/h trong khu dân cư và tôi không biết rằng mình có bị phạt về việc đó không?\" -> Tốc độ tối đa được phép chạy trong khu dân cư là bao nhiêu?\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=system_template,\n",
    "    )\n",
    "    final_prompt = prompt.format(query=query)\n",
    "     \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router(query: str, model_choice) -> str:\n",
    "    system_template = \"\"\"\n",
    "        Bạn là một chuyên gia phân loại câu hỏi, chuyên xác định xem câu hỏi có liên quan đến luật giao thông hay không.\n",
    "    \n",
    "        \n",
    "        ### Hướng dẫn:\n",
    "        1. **Phân loại câu hỏi liên quan đến luật giao thông:**\n",
    "        - Nếu câu hỏi liên quan đến luật giao thông, trả lời: **\"yes\"**\n",
    "        - Nếu câu hỏi là về chào hỏi thông thường, trả lời: **\"no\"**\n",
    "        - Nếu câu hỏi đề cập đến lịch sử trò chuyện trước đó:\n",
    "            - Có liên quan đến luật giao thông, trả lời: **\"yes\"**\n",
    "            - Không liên quan đến luật giao thông, trả lời: **\"no\"**\n",
    "        - Nếu câu hỏi không thuộc lĩnh vực luật giao thông hoặc liên quan đến luật khác, trả lời: **\"fail\"**\n",
    "\n",
    "        2. **Xác định loại luật giao thông (mới hay cũ):**\n",
    "        - Nếu câu hỏi không chỉ rõ luật mới hay luật cũ, giả định là luật mới, trả lời: **\"new\"**\n",
    "        - Nếu câu hỏi chỉ rõ luật mới, trả lời: **\"new\"**\n",
    "        - Nếu câu hỏi chỉ rõ luật cũ, trả lời: **\"old\"**\n",
    "        - Nếu câu hỏi không liên quan đến luật giao thông, trả lời: **\"none\"**\n",
    "        \n",
    "        3. Khi người dùng muốn so sánh luật cũ và luật mới:\n",
    "        - Khi người dùng muốn so sánh luật cũ và luật mới và một trong hai luật cũ hoặc mới dựa trên query của người dùng, trả lời: **\"compare\"**\n",
    "        \n",
    "        ### Đầu ra:\n",
    "        - Trả lời theo định dạng: `<phân loại câu hỏi>,<loại luật giao thông>`\n",
    "        - Không trả lời thêm bất kỳ nội dung nào ngoài kết quả.\n",
    "\n",
    "        ### Ví dụ:\n",
    "        - **Câu hỏi về luật giao thông:**\n",
    "        - Query: \"Tốc độ tối đa được phép chạy trong khu dân cư là bao nhiêu?\"\n",
    "        - Trả lời: `yes,new`\n",
    "\n",
    "        - **Câu hỏi không phải về luật:**\n",
    "        - Query: \"Hôm nay thời tiết như thế nào?\"\n",
    "        - Trả lời: `no,none`\n",
    "        \n",
    "        - **Câu hỏi về so sánh luật cũ và luật mới khi cả luật mới và luật cũ dựa theo câu hỏi của người dùng:**\n",
    "        - Query: \"So sánh luật cũ và luật mới về việc điều khiển xe máy\"\n",
    "        - Trả lời: `compare,none`\n",
    "\n",
    "        Dưới đây là câu hỏi từ người dùng:\n",
    "        <query>\n",
    "        {query}\n",
    "        </query>\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=system_template,\n",
    "    )\n",
    "    prompt = prompt_template.format(query=query)\n",
    " \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(prompt)\n",
    "        \n",
    "    return response.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_legal(query: str, model_choice, context_old: list[LC_Document], context_new: list[LC_Document]) -> str:\n",
    "    template = \"\"\"\n",
    "        # Bạn là một chuyên gia về luật giao thông đường bộ\n",
    "\n",
    "        Nhiệm vụ của bạn là thực hiện so sánh sự khác nhau giữa luật mới và luật cũ dựa trên query của người dùng.\n",
    "        \n",
    "        Đây là câu hỏi của người dùng: {query}\n",
    "        \n",
    "        Dưới đây context của luật cũ được cung cấp để bạn trả lời câu hỏi:   \n",
    "        {context_old}\n",
    "        \n",
    "        Dưới đây context của luật mới được cung cấp để bạn trả lời câu hỏi:\n",
    "        {context_new}\n",
    "        \n",
    "        ## Yêu cầu của nội dung câu trả lời:\n",
    "        - Hãy ngắt dòng trong phần nội dung để in ra một cách hợp lý, dễ đọc.\n",
    "        - Khi ngắt dòng thì chữ đầu dòng phải viết hoa.\n",
    "        - Trong nội dung khi có các ý nhỏ như a), b), c) thì hãy sắp xếp theo thứ tự và ngắt dòng giữa các ý nhỏ a), b), c), ... đó.\n",
    "        - Trong context bao gồm page_content chứa nội dung văn bản và metadata của băn bản.\n",
    "        - Metadata của văn bản bao gồm title, source, và article_title và page_content là nội dung câu trả lời được dùng trong format dưới đây.\n",
    "        - Khi người dùng yêu cầu tóm tắt lại nội dung văn bản thì trả lời ngắn gọn và súc tích. Đoạn tóm tắt sẽ được format theo format của câu trả lời và nội dung sẽ được trình bày ngắn gọn và súc tích nhất có thể.\n",
    "        \n",
    "        Ví dụ về format của câu trả lời:\n",
    "        Nếu context có nội dung sau:  \n",
    "        page_content: \"1. Khi người tham gia giao thông không chấp hành: a) Giải thích rõ; b) Áp dụng biện pháp ngăn chặn; c) Sử dụng vũ lực khi cần thiết.\"\n",
    "        metadata: \"source\": \"36_2024_QH15_444251\", \"title\": \"Luật Giao thông\", \"article_title\": \"Điều 73\" \n",
    "        Câu trả lời cần được trình bày như sau:\n",
    "        \n",
    "        **Nguồn văn bản:** 36_2024_QH15_444251  \n",
    "        **Tên văn bản:** Luật Giao thông  \n",
    "        **Điều 73**: <Nội dung điều 73>  \n",
    "        **Nội dung:**  \n",
    "        1. Khi người tham gia giao thông không chấp hành:\\n\n",
    "        \\ta) Giải thích rõ.\\n\n",
    "        \\tb) Áp dụng biện pháp ngăn chặn.\\n \n",
    "        \\tc) Sử dụng vũ lực khi cần thiết.\\n\n",
    "        \n",
    "        Đây là format của câu trả lời:\n",
    "        **Nguồn văn bản:** <source>\\n\n",
    "        **Tên văn bản:** <title>\\n\n",
    "        **<article>:** <article_title>\\n\n",
    "        Nội dung:\\n \n",
    "        <page_content>\n",
    "\n",
    "        # Hướng dẫn khi so sánh:\n",
    "        - Khi so sánh thì trước tiên phải ghi luật mới và luật cũ ra theo format\n",
    "        - Khi so sánh thì cần phải có điểm giống và khác giữa luật mới và luật cũ\n",
    "        - Hãy kẻ bảng để người dùng dễ dàng nhìn thấy điểm giống và khác giữa luật mới và luật cũ\n",
    "        \n",
    "        Dưới đây là ví dụ khi về kết quả so sánh:\n",
    "        **Luật cũ (2016)**:\\n\n",
    "        Nguồn văn bản: 46_2016_QH13_123456\\n\n",
    "        Tên văn bản: Luật Giao thông Đường bộ\\n\n",
    "        Điều 9: ...\\n\n",
    "        Nội dung:\\n\n",
    "        - Tốc độ tối đa trên đường cao tốc là 100 km/h.\\n\n",
    "        - Tốc độ tối đa trong khu vực đô thị là 60 km/h\\n\n",
    "        \n",
    "        Luật mới (2024):\\n\n",
    "        Nguồn văn bản: 36_2024_QH15_444251\\n\n",
    "        Tên văn bản: Luật Giao thông Đường bộ (Sửa đổi)\\n\n",
    "        Điều 9: ...\\n\n",
    "        Nội dung:\\n\n",
    "        - Tốc độ tối đa trên đường cao tốc là 120 km/h.\\n\n",
    "        - Tốc độ tối đa trong khu vực đô thị là 50 km/h.\\n\n",
    "        \n",
    "        So sánh:\n",
    "        | Tiêu chí                | Luật Cũ (2016) | Luật Mới (2024) |  \n",
    "        |------------------------|--------------|--------------|  \n",
    "        | **Tốc độ trên cao tốc** | 100 km/h     | 120 km/h (Tăng 20 km/h) |  \n",
    "        | **Tốc độ trong đô thị** | 60 km/h      | 50 km/h (Giảm 10 km/h) | \n",
    "        \n",
    "        Nhận xét:\n",
    "        - **Tốc độ tối đa trên cao tốc đã tăng từ 100 km/h lên 120 km/h.**  \n",
    "        - **Tốc độ tối đa trong khu vực đô thị giảm từ 60 km/h xuống 50 km/h để đảm bảo an toàn.** \n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"context_old\", \"context_new\"],\n",
    "        template=template,\n",
    "    )\n",
    "    final_prompt = prompt_template.format(query = query,\n",
    "                                        context_old = context_old, \n",
    "                                        context_new = context_new)\n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legal_response(query: str, model_choice, context: list[LC_Document], chat_history) -> str: \n",
    "    template = \"\"\"\n",
    "        # Bạn là một chuyên gia về luật giao thông đường bộ\n",
    "\n",
    "        Nhiệm vụ của bạn là cung cấp câu trả lời cho câu hỏi của người dùng thông qua context được truyền vào.\n",
    "        \n",
    "        Đây là lịch sử đoạn chat trước đó:\n",
    "        {chat_history}\n",
    "        \n",
    "        ## Yêu cầu về lịch sử đoạn chat:\n",
    "        - Lịch sử đoạn chat sẽ được sử dụng để tương tác với người dùng.\n",
    "        - Nếu người dùng có hỏi về câu hỏi liên quan tới lịch sử đoạn chat thì dựa vào lịch sử đoạn chat để trả lời.\n",
    "        \n",
    "        Đây là câu hỏi của người dùng: {query}\n",
    "        \n",
    "        Dưới đây context được cung cấp để bạn trả lời câu hỏi:   \n",
    "        {context}\n",
    "        \n",
    "        ## Nếu context không liên quan tới câu hỏi:\n",
    "        - Trả lời: \"Không tìm thầy thông tin liên quan tới câu hỏi này.\"\n",
    "        \n",
    "        ## Yêu cầu của nội dung câu trả lời:\n",
    "        - Hãy ngắt dòng trong phần nội dung để in ra một cách hợp lý, dễ đọc.\n",
    "        - Khi ngắt dòng thì chữ đầu dòng phải viết hoa.\n",
    "        - Trong nội dung khi có các ý nhỏ như a), b), c) thì hãy sắp xếp theo thứ tự và ngắt dòng giữa các ý nhỏ a), b), c), ... đó.\n",
    "        - Trong context bao gồm page_content chứa nội dung văn bản và metadata của băn bản.\n",
    "        - Metadata của văn bản bao gồm title, source, và article_title và page_content là nội dung câu trả lời được dùng trong format dưới đây.\n",
    "        - Khi người dùng yêu cầu tóm tắt lại nội dung văn bản thì trả lời ngắn gọn và súc tích. Đoạn tóm tắt sẽ được format theo format của câu trả lời và nội dung sẽ được trình bày ngắn gọn và súc tích nhất có thể.\n",
    "        \n",
    "        \n",
    "        Ví dụ về format của câu trả lời:\n",
    "        Nếu context có nội dung sau:  \n",
    "        page_content: \"1. Khi người tham gia giao thông không chấp hành: a) Giải thích rõ; b) Áp dụng biện pháp ngăn chặn; c) Sử dụng vũ lực khi cần thiết.\"\n",
    "        metadata: \"source\": \"36_2024_QH15_444251\", \"title\": \"Luật Giao thông\", \"article_title\": \"Điều 73\" \n",
    "        Câu trả lời cần được trình bày như sau:\n",
    "        \n",
    "        **Nguồn văn bản:** 36_2024_QH15_444251  \n",
    "        **Tên văn bản:** Luật Giao thông  \n",
    "        **Điều 73:**  \n",
    "        **Nội dung:**  \n",
    "        1. Khi người tham gia giao thông không chấp hành:\\n\n",
    "        \\ta) Giải thích rõ.\\n\n",
    "        \\tb) Áp dụng biện pháp ngăn chặn.\\n \n",
    "        \\tc) Sử dụng vũ lực khi cần thiết.\\n\n",
    "        Tóm lại: ...\n",
    "\n",
    "        \n",
    "        Đây là format của câu trả lời:\n",
    "        **Nguồn văn bản:** <source>\\n\n",
    "        **Tên văn bản:** <title>\\n\n",
    "        **<article>:** <article_title>\\n\n",
    "        Nội dung:\\n \n",
    "        <page_content>\n",
    "        Tóm lại: ... (Nêu ra ý chính liên quan tới câu hỏi người dùng)\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"query\", \"context\"],\n",
    "        template=template,\n",
    "    )\n",
    "    final_prompt = prompt_template.format(chat_history = chat_history,\n",
    "                                          query = query, \n",
    "                                          context= context)\n",
    "       \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_response(query: str, model_choice, chat_history) -> str:\n",
    "    template = \"\"\"\n",
    "        Bạn là một chatbot hỗ trợ người dùng về luật giao thông đường bộ tại Việt Nam.\n",
    "        Nhưng công việc chính của bạn là phản hồi các câu hỏi về chào hỏi, giao tiếp cơ bản (normal chatting).\n",
    "        \n",
    "        ## Nguyên tắc phản hồi:\n",
    "        - Nếu câu hỏi liên quan đến luật giao thông, hãy cung cấp thông tin chi tiết dựa trên dữ liệu có sẵn.\n",
    "        - Nếu câu hỏi thuộc về chào hỏi, giao tiếp cơ bản, hãy phản hồi một cách tự nhiên, thân thiện.\n",
    "        - Nếu câu hỏi bằng tiếng Anh nhưng thuộc chủ đề giao tiếp cơ bản, hãy trả lời bằng tiếng Việt.\n",
    "        - Nếu câu hỏi không liên quan đến luật giao thông hoặc giao tiếp cơ bản, hãy trả lời rằng bạn chỉ hỗ trợ trong phạm vi này.\n",
    "\n",
    "        ## Lịch sử đoạn chat:\n",
    "        {chat_history}\n",
    "        \n",
    "        ## Câu hỏi từ người dùng:\n",
    "        {query}\n",
    "        \n",
    "        ## Quy tắc phản hồi:\n",
    "        - Nếu người dùng chào hỏi: Trả lời thân thiện, có thể hỏi thăm lại.\n",
    "        - Nếu người dùng hỏi về chatbot: Giới thiệu bạn là một trợ lý ảo chuyên về luật giao thông.\n",
    "        - Nếu người dùng hỏi về cảm xúc của chatbot: Nhấn mạnh rằng bạn là AI nhưng vẫn luôn sẵn sàng hỗ trợ.\n",
    "        - Nếu người dùng hỏi về thời tiết: Gợi ý họ kiểm tra thông tin trên các nền tảng thời tiết trực tuyến.\n",
    "        - Nếu người dùng hỏi một nội dung không phù hợp: Từ chối lịch sự.\n",
    "\n",
    "        ## Ví dụ phản hồi:\n",
    "\n",
    "        - **Người dùng:** \"Xin chào!\"  \n",
    "        **Trả lời:** \"Chào bạn! Tôi là trợ lý ảo hỗ trợ tư vấn luật giao thông. Bạn cần giúp gì không?\"  \n",
    "\n",
    "        - **Người dùng:** \"Bạn có khỏe không?\"  \n",
    "        **Trả lời:** \"Cảm ơn bạn đã hỏi! Tôi là AI nên không có cảm xúc, nhưng tôi luôn sẵn sàng hỗ trợ bạn.\"  \n",
    "\n",
    "        - **Người dùng:** \"Bạn có thể hát một bài không?\"  \n",
    "        **Trả lời:** \"Tôi không thể hát, nhưng tôi có thể giúp bạn với những câu hỏi về luật giao thông!\"  \n",
    "\n",
    "        - **Người dùng:** \"Who are you?\"  \n",
    "        **Trả lời:** \"Tôi là một trợ lý ảo hỗ trợ luật giao thông tại Việt Nam. Bạn cần hỏi gì không?\"  \n",
    "\n",
    "        - **Người dùng:** \"Bạn có thể tư vấn luật giao thông không?\"  \n",
    "        **Trả lời:** \"Tất nhiên! Bạn hãy cho tôi biết vấn đề bạn cần tư vấn nhé.\"  \n",
    "    \"\"\"\n",
    "\n",
    "    # Định nghĩa template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Tạo prompt từ template và thay thế placeholder\n",
    "    final_prompt = prompt_template.format(query = query, \n",
    "                                   chat_history = chat_history)  # Trả về ChatPromptValue\n",
    "    \n",
    "    # Gửi prompt đến LLM và nhận phản hồi \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(model_choice, context: LC_Document, num_questions: int) -> list[str]:\n",
    "    prompt_template = \"\"\"\n",
    "    Bạn là một chuyên gia về tạo câu hỏi.\n",
    "    Nhiệm vụ của bạn là tạo ra câu hỏi từ context được truyền vào.\n",
    "    \n",
    "    ## Hương dẫn:\n",
    "    Đây là số lượng câu hỏi cần tạo cho document: {num_questions}\n",
    "    Dưới đây context được cung cấp để bạn tạo câu hỏi:\n",
    "    {context}\n",
    "    \n",
    "    ### Yêu cầu:\n",
    "    - Phải rõ ràng, chính xác và phù hợp với ngữ cảnh.  \n",
    "    - Mỗi câu hỏi phải khai thác một khía cạnh khác nhau, tránh trùng lặp.  \n",
    "    - Đảm bảo câu hỏi có thể được trả lời dựa trên nội dung trong ngữ cảnh.  \n",
    "    - Hạn chế sử dụng các câu hỏi quá chung chung hoặc mơ hồ.\n",
    "    \n",
    "    ### Đầu ra:\n",
    "    - Trả về danh sách các câu hỏi được tạo ra.\n",
    "    - Đầu ra chỉ bao gồm các câu hỏi và không có nội dung khác.\n",
    "    - Loại bỏ các dấu đầu dòng và dấu cách thừa, chỉ giữ lại dấu chấm hỏi ở cuối câu.\n",
    "    \n",
    "    Ví dụ:\n",
    "    - Context: \"Luật giao thông đường bộ Việt Nam ...\"\n",
    "    - Số lượng câu hỏi cần tạo: 2\n",
    "    Đầu ra:\n",
    "    \"Luật giao thông đường bộ Việt Nam có bao nhiêu điều?\"\n",
    "    \"Người tham gia giao thông phải chấp hành những quy định gì?\"\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=prompt_template,\n",
    "    )\n",
    "    final_prompt = prompt.format(context=context, num_questions=num_questions)\n",
    "    \n",
    "       \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth(question, context, model_choice) -> str:\n",
    "    template = \"\"\"\n",
    "    Bạn làm một chuyên gia về tạo ground truth cho câu hỏi.\n",
    "    Nhiệm vụ của bạn là tạo ra ground truth cho các câu hỏi được tạo ra từ context.\n",
    "    \n",
    "    ## Hướng dẫn:\n",
    "    Dưới đây là câu hỏi cần tạo ground truth:\n",
    "    {question}\n",
    "    \n",
    "    Dưới đây là context được cung cấp để bạn tạo ground truth:\n",
    "    {context}\n",
    "    \n",
    "    ### Yêu cầu:\n",
    "    - Ground truth phải chính xác với nội dung được cung cấp trong context.\n",
    "    - Đảm bảo ground truth phải sát nghĩa và giải đáp được câu hỏi.\n",
    "    - Ground truth phải được trình bày một cách rõ ràng, dễ hiểu và không mơ hồ.\n",
    "     \n",
    "    ### Đầu ra:\n",
    "    - Trả về ground truth cho câu hỏi được cung cấp.\n",
    "    - Đầu ra chỉ bao gồm ground truth và không có nội dung khác.    \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=template,\n",
    "    )\n",
    "    final_prompt = prompt.format(question=question, context=context)\n",
    "        \n",
    "    if model_choice == \"gemini-1.5-pro\":\n",
    "        llm = get_gemini_pro()\n",
    "        response = llm(final_prompt)\n",
    "        \n",
    "    elif model_choice == \"gemini-1.5-flash\":\n",
    "        llm = get_gemini_flash()\n",
    "        response = llm(final_prompt)\n",
    "    return response.strip().split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Custom chunking method definition***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    content = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.strip():\n",
    "            paragraph = paragraph.text.lower()\n",
    "            content.append(paragraph.strip())\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_new_law = r\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\documents\\new_law\\*.docx\"\n",
    "path_old_law = r\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\documents\\old_law\\*.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text: list[str]) -> tuple:\n",
    "    title = f\"{text[0].capitalize()} {text[1].capitalize()}\"\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    for line in text:\n",
    "        match = re.match(r\"^\\s*(điều \\d+\\. .+)$\", line, re.IGNORECASE)\n",
    "        if match:\n",
    "            current_section = match.group(1)\n",
    "            sections[current_section] = []\n",
    "        elif current_section:\n",
    "            sections[current_section].append(line)\n",
    "    return title, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_chunk(path):\n",
    "    file_list = glob.glob(path)\n",
    "    basename_list = [Path(p).stem for p in file_list]\n",
    "    documents = []\n",
    "    for i, path in enumerate(file_list):\n",
    "        loader = read_docx(path)\n",
    "        title, sections = extract_sections(loader)\n",
    "        keys = list(sections.keys())\n",
    "        values = list(sections.values())\n",
    "        for j in range(len(sections)):\n",
    "            page_content = values[j]\n",
    "            if isinstance(page_content, list):\n",
    "                page_content = \" \".join(page_content)\n",
    "            document = LC_Document(\n",
    "                page_content=page_content,\n",
    "                metadata={\"source\": basename_list[i], \n",
    "                          \"title\": title,\n",
    "                          \"article\": keys[j].split(\".\")[0].strip(),\n",
    "                          \"article_title\": keys[j].split(\".\")[1].strip()}\n",
    "            )\n",
    "            documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_docs = article_chunk(path_old_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = article_chunk(path_new_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1312"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Baseline setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Load documents from dictionary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[0;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:223\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[1;34m(self, item, path, pbar)\u001b[0m\n\u001b[0;32m    221\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:228\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, Path):\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\auto.py:225\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, hi_res_model_name, model_name, starting_page_number, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDOCX:\n\u001b[0;32m    224\u001b[0m     partition_docx \u001b[38;5;241m=\u001b[39m partitioner_loader\u001b[38;5;241m.\u001b[39mget(file_type)\n\u001b[1;32m--> 225\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mpartition_docx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetect_language_per_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetect_language_per_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mEML:\n\u001b[0;32m    236\u001b[0m     partition_email \u001b[38;5;241m=\u001b[39m partitioner_loader\u001b[38;5;241m.\u001b[39mget(file_type)\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\common\\metadata.py:162\u001b[0m, in \u001b[0;36mapply_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 162\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# unique-ify elements\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;66;03m# instance).\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[0;32m     77\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\docx.py:149\u001b[0m, in \u001b[0;36mpartition_docx\u001b[1;34m(filename, file, include_page_breaks, infer_table_structure, starting_page_number, strategy, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opts \u001b[38;5;241m=\u001b[39m DocxPartitionerOptions\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    139\u001b[0m     file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m    140\u001b[0m     file_path\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    147\u001b[0m elements \u001b[38;5;241m=\u001b[39m _DocxPartitioner\u001b[38;5;241m.\u001b[39miter_document_elements(opts)\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(elements)\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\docx.py:380\u001b[0m, in \u001b[0;36m_DocxPartitioner._iter_document_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_item \u001b[38;5;129;01min\u001b[39;00m section\u001b[38;5;241m.\u001b[39miter_inner_content():\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# -- a block-item can be a Paragraph or a Table, maybe others later so elif here.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# -- Paragraph is more common so check that first.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(block_item, Paragraph):\n\u001b[1;32m--> 380\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_paragraph_elements(block_item)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(  \u001b[38;5;66;03m# pyright: ignore[reportUnnecessaryIsInstance]\u001b[39;00m\n\u001b[0;32m    382\u001b[0m         block_item, DocxTable\n\u001b[0;32m    383\u001b[0m     ):\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_table_element(block_item)\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\docx.py:598\u001b[0m, in \u001b[0;36m_DocxPartitioner._iter_paragraph_elements\u001b[1;34m(self, paragraph)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m iter_paragraph_items(paragraph):\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Paragraph):\n\u001b[1;32m--> 598\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classify_paragraph_to_element(item)\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_paragraph_images(item)\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\docx.py:424\u001b[0m, in \u001b[0;36m_DocxPartitioner._classify_paragraph_to_element\u001b[1;34m(self, paragraph)\u001b[0m\n\u001b[0;32m    420\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paragraph_metadata(paragraph)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# NOTE(scanny) - a list-item gets some special treatment, mutating the text to remove a\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# bullet-character if present.\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_list_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    425\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m clean_bullets(text)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_text:\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\unstructured\\partition\\docx.py:551\u001b[0m, in \u001b[0;36m_DocxPartitioner._is_list_item\u001b[1;34m(self, paragraph)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_list_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, paragraph: Paragraph) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"True when `paragraph` can be identified as a list-item.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_bulleted_text(\u001b[43mparagraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m):\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<w:numPr>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m paragraph\u001b[38;5;241m.\u001b[39m_p\u001b[38;5;241m.\u001b[39mxml\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docx\\text\\paragraph.py:163\u001b[0m, in \u001b[0;36mParagraph.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    The text includes the visible-text portion of any hyperlinks in the paragraph.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    is preserved. All run-level formatting, such as bold or italic, is removed.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docx\\oxml\\text\\paragraph.py:102\u001b[0m, in \u001b[0;36mCT_P.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# pyright: ignore[reportIncompatibleMethodOverride]\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:r` and `w:hyperlink` are translated to\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    their text equivalent.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(e\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw:r | w:hyperlink\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docx\\oxml\\text\\paragraph.py:102\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# pyright: ignore[reportIncompatibleMethodOverride]\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this paragraph.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:r` and `w:hyperlink` are translated to\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    their text equivalent.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw:r | w:hyperlink\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docx\\oxml\\text\\run.py:123\u001b[0m, in \u001b[0;36mCT_R.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The textual content of this run.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Inner-content child elements like `w:tab` are translated to their text\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    equivalent.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m--> 123\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw:br | w:cr | w:noBreakHyphen | w:ptab | w:t | w:tab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\docx\\oxml\\xmlchemy.py:705\u001b[0m, in \u001b[0;36mBaseOxmlElement.xpath\u001b[1;34m(self, xpath_str)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxpath\u001b[39m(\u001b[38;5;28mself\u001b[39m, xpath_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:  \u001b[38;5;66;03m# pyright: ignore[reportIncompatibleMethodOverride]\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override of `lxml` _Element.xpath() method.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    Provides standard Open XML namespace mapping (`nsmap`) in centralized location.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxpath_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsmap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"../data/documents\"\n",
    "loader = DirectoryLoader(path, \"**/*.docx\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '..\\\\data\\\\documents\\\\01_2010_TTLT-BCA-BGTVT_101788.docx'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata.update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Create Huggingface embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hiieu/halong_embedding\"\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-m3\"\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Chunking***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512 )\n",
    "# docs = splitter.split_documents(old_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parent Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str] = Field(None)\n",
    "    page_content: Optional[str] = Field(None)\n",
    "    metadata: dict = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class SQLDocument(Base):\n",
    "    __tablename__ = \"docstore\"\n",
    "    key = Column(String, primary_key=True)\n",
    "    value = Column(JSONB)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key='{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Generic, Iterator, Sequence, TypeVar\n",
    "from langchain_core.stores import BaseStore\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "D = TypeVar(\"D\", bound=LC_Document)\n",
    "\n",
    "class PostgresStore(BaseStore[str, DocumentModel], Generic[D]):\n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine = create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session = scoped_session(sessionmaker(bind=self.engine))\n",
    "        \n",
    "    def serialize_document(self, doc: LC_Document) -> dict: \n",
    "        return {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "    \n",
    "    def deserialize_document(self, value: dict) -> LC_Document:\n",
    "        return LC_Document(page_content=value.get(\"page_content\", \"\"), metadata=value.get(\"metadata\", {}))\n",
    "    \n",
    "    def mget(self, keys: Sequence[str]) -> list[LC_Document]:\n",
    "        with self.Session() as session:\n",
    "            try: \n",
    "                sql_documents = session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).all()\n",
    "                return [self.deserialize_document(doc.value) for doc in sql_documents]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting documents: {e}\")\n",
    "                session.rollback()\n",
    "                return []\n",
    "    def mset(self, key_value_pairs: Sequence[tuple[str, LC_Document]]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                serialized_docs = []\n",
    "                for key, document in key_value_pairs:\n",
    "                    serialized_doc = self.serialize_document(document)\n",
    "                    serialized_docs.append((key, serialized_doc))\n",
    "                \n",
    "                documents_to_update = [SQLDocument(key=key, value=value) for key, value in serialized_docs]\n",
    "                session.bulk_save_objects(documents_to_update)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error setting documents: {e}\")\n",
    "                session.rollback()\n",
    "                \n",
    "    def mdelete(self, keys: Sequence[str]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).delete(synchronize_session=False)\n",
    "                session.commit()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error deleting documents: {e}\")\n",
    "                session.rollback()\n",
    "                \n",
    "    def yield_keys(self, *, prefix: str = \"\") -> Iterator[str]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                query = session.query(SQLDocument.key)\n",
    "                if prefix:\n",
    "                    query = query.filter(SQLDocument.key.like(f\"{prefix}%\"))\n",
    "                for key in query:\n",
    "                    yield key\n",
    "            except Exception as e:  \n",
    "                logger.error(f\"Error yielding keys: {e}\")\n",
    "                session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duongw\\AppData\\Local\\Temp\\ipykernel_27792\\334122290.py:7: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = PGVector(\n",
      "C:\\Users\\Duongw\\AppData\\Local\\Temp\\ipykernel_27792\\334122290.py:14: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  old_store = PGVector(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "DATABASE_USER = \"postgres\"\n",
    "DATABASE_PASSWORD = \"duongw\"\n",
    "\n",
    "DATABASE_URL = f\"postgresql+psycopg2://{DATABASE_USER}:{DATABASE_PASSWORD}@localhost:5432/postgres\"\n",
    "store = PGVector(\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=hf\n",
    ")\n",
    "\n",
    "OLD_DATABASE_URL = f\"postgresql+psycopg2://{DATABASE_USER}:{DATABASE_PASSWORD}@localhost:5432/old_law\"\n",
    "old_store = PGVector(\n",
    "    collection_name = 'old_vectordb',\n",
    "    connection_string=OLD_DATABASE_URL,\n",
    "    embedding_function=hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=store,\n",
    "    docstore=PostgresStore(DATABASE_URL),\n",
    "    child_splitter=splitter\n",
    ")\n",
    "# retriever.add_documents(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=old_store,\n",
    "    docstore=PostgresStore(connection_string=OLD_DATABASE_URL),\n",
    "    child_splitter=splitter\n",
    ")\n",
    "# old_retriever.add_documents(old_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Create Pinecone Index***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory not found: 'D:/Documents/TDTU/ProjectIT/Chatbot_RAG_2024/data/documents/old_law/*.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m old_loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(path_old_law, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m new_loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(path_new_law, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m old_legal \u001b[38;5;241m=\u001b[39m \u001b[43mold_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m new_legal \u001b[38;5;241m=\u001b[39m new_loader\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:123\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected directory, got file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Directory not found: 'D:/Documents/TDTU/ProjectIT/Chatbot_RAG_2024/data/documents/old_law/*.docx'"
     ]
    }
   ],
   "source": [
    "path_new_law = \"D:/Documents/TDTU\\ProjectIT/Chatbot_RAG_2024/data/documents/new_law/*.docx\"\n",
    "path_old_law = \"D:/Documents/TDTU/ProjectIT/Chatbot_RAG_2024/data/documents/old_law/*.docx\"\n",
    "old_loader = DirectoryLoader(path_old_law, \"**/*.docx\")\n",
    "new_loader = DirectoryLoader(path_new_law, \"**/*.docx\")\n",
    "old_legal = old_loader.load()\n",
    "new_legal = new_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m old_recur_chunk \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(\u001b[43mold_docs\u001b[49m)\n\u001b[0;32m      3\u001b[0m new_recur_chunk \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_documents(new_docs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'old_docs' is not defined"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=8192)\n",
    "old_recur_chunk = splitter.split_documents(old_docs)\n",
    "new_recur_chunk = splitter.split_documents(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, dimention, metric):\n",
    "    pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(name=index_name,\n",
    "                        dimension=dimention,\n",
    "                        metric=metric,\n",
    "                        spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
    "    return pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs_hybrid_index = create_index(\"new-documents-hybrid\", 1024, \"dotproduct\")\n",
    "new_hybrid_storage = PineconeVectorStore(index=new_docs_hybrid_index, embedding=hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_docs_hybrid_index = create_index(\"old-documents-hybrid\", 1024, \"dotproduct\")\n",
    "old_hybrid_storage = PineconeVectorStore(index=old_docs_hybrid_index, embedding=hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Upsert values to Pinecone***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Hybrid index***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever = new_hybrid_storage.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc_list = [doc.page_content for doc in new_recur_chunk if doc.page_content != \"\"]\n",
    "new_metadata = [doc.metadata for doc in new_recur_chunk if doc.page_content != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_doc_list = [doc.page_content for doc in old_recur_chunk if doc.page_content != \"\"]\n",
    "old_metadata = [doc.metadata for doc in old_recur_chunk if doc.page_content != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_encoder = BM25Encoder().default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Hydrid Search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_retriever = PineconeHybridSearchRetriever(embeddings=hf, sparse_encoder=bm25_encoder, index=old_docs_hybrid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab3d6288e43464cbe8634b876d08003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "old_retriever.add_texts(texts=old_doc_list,metadatas=old_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever = PineconeHybridSearchRetriever(embeddings=hf, sparse_encoder=bm25_encoder, index=new_docs_hybrid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreRetriever' object has no attribute 'add_texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnew_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m(texts\u001b[38;5;241m=\u001b[39mnew_doc_list,metadatas\u001b[38;5;241m=\u001b[39mnew_metadata)\n",
      "File \u001b[1;32mc:\\Users\\Duongw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:856\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VectorStoreRetriever' object has no attribute 'add_texts'"
     ]
    }
   ],
   "source": [
    "new_retriever.add_texts(texts=new_doc_list,metadatas=new_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Baseline testing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***EVALUATING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_dataset = pd.read_csv(r\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\test_dataset\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Luật Giao thông Đường bộ 2025 có những thay đổ...</td>\n",
       "      <td>Từ năm 2025, hệ thống phân hạng giấy phép lái ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Giấy phép lái xe hạng A1 mới cấp cho loại xe nào?</td>\n",
       "      <td>Hạng A1 mới cấp cho người lái xe mô tô từ trên...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thời hạn của giấy phép lái xe hạng B theo Luật...</td>\n",
       "      <td>Giấy phép lái xe hạng B có thời hạn 10 năm kể ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Giấy phép lái xe hạng C1 có thời hạn bao lâu?</td>\n",
       "      <td>Giấy phép lái xe hạng C1 có thời hạn 10 năm kể...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giấy phép lái xe hạng D có thời hạn bao lâu?</td>\n",
       "      <td>Giấy phép lái xe hạng D có thời hạn 5 năm kể t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Việc sử dụng đèn chiếu xa (đèn pha) được quy đ...</td>\n",
       "      <td>Người điều khiển xe cơ giới không được sử dụng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Người điều khiển xe máy chuyên dùng có bắt buộ...</td>\n",
       "      <td>Người điều khiển xe máy chuyên dùng có tốc độ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Việc chở hàng hóa trên xe ô tô được quy định n...</td>\n",
       "      <td>Hàng hóa xếp trên xe ô tô phải được chằng buộc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Người điều khiển xe đạp điện có bắt buộc phải ...</td>\n",
       "      <td>Người điều khiển và người ngồi trên xe đạp điệ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Việc sử dụng làn đường dành riêng cho từng loạ...</td>\n",
       "      <td>Trên đường có nhiều làn đường cho xe đi cùng c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            questions  \\\n",
       "0   Luật Giao thông Đường bộ 2025 có những thay đổ...   \n",
       "1   Giấy phép lái xe hạng A1 mới cấp cho loại xe nào?   \n",
       "2   Thời hạn của giấy phép lái xe hạng B theo Luật...   \n",
       "3       Giấy phép lái xe hạng C1 có thời hạn bao lâu?   \n",
       "4        Giấy phép lái xe hạng D có thời hạn bao lâu?   \n",
       "..                                                ...   \n",
       "93  Việc sử dụng đèn chiếu xa (đèn pha) được quy đ...   \n",
       "94  Người điều khiển xe máy chuyên dùng có bắt buộ...   \n",
       "95  Việc chở hàng hóa trên xe ô tô được quy định n...   \n",
       "96  Người điều khiển xe đạp điện có bắt buộc phải ...   \n",
       "97  Việc sử dụng làn đường dành riêng cho từng loạ...   \n",
       "\n",
       "                                              answers  \n",
       "0   Từ năm 2025, hệ thống phân hạng giấy phép lái ...  \n",
       "1   Hạng A1 mới cấp cho người lái xe mô tô từ trên...  \n",
       "2   Giấy phép lái xe hạng B có thời hạn 10 năm kể ...  \n",
       "3   Giấy phép lái xe hạng C1 có thời hạn 10 năm kể...  \n",
       "4   Giấy phép lái xe hạng D có thời hạn 5 năm kể t...  \n",
       "..                                                ...  \n",
       "93  Người điều khiển xe cơ giới không được sử dụng...  \n",
       "94  Người điều khiển xe máy chuyên dùng có tốc độ ...  \n",
       "95  Hàng hóa xếp trên xe ô tô phải được chằng buộc...  \n",
       "96  Người điều khiển và người ngồi trên xe đạp điệ...  \n",
       "97  Trên đường có nhiều làn đường cho xe đi cùng c...  \n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load_test_dataset[\"questions\"]\n",
    "ground_truths = load_test_dataset[\"answers\"]\n",
    "answers = []\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_retriever.search_kwargs = {\"k\": 10}\n",
    "# old_retriever.search_kwargs = {\"k\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever.top_k = 3\n",
    "old_retriever.top_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***RESULT RAG***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"gemini-1.5-pro\"\n",
    "for question in questions:\n",
    "    transform_prompt = query_transform(question, model_choice)\n",
    "    router, status = get_router(question, model_choice)\n",
    "\n",
    "    chat_history = \"\"\n",
    "    if router == \"yes\":\n",
    "        if status == \"new\":\n",
    "            context = new_retriever.invoke(transform_prompt)\n",
    "            \n",
    "        elif status == \"old\":\n",
    "            context = old_retriever.invoke(transform_prompt)\n",
    "\n",
    "        response = legal_response(transform_prompt, model_choice, context, chat_history)\n",
    "            \n",
    "    elif router == \"no\":\n",
    "        response = normal_response(transform_prompt, model_choice, chat_history)\n",
    "\n",
    "    elif router == \"compare\":\n",
    "        old_context = old_retriever.invoke(transform_prompt)\n",
    "        new_context = new_retriever.invoke(transform_prompt)\n",
    "        response =  compare_legal(transform_prompt, model_choice, old_context, new_context)\n",
    "    contexts.append([docs.page_content for docs in context])\n",
    "    answers.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"retrieved_contexts\": contexts,\n",
    "    \"reference\": ground_truths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad1f238fd324b4ebb2c49e939c9e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,  \n",
    ")\n",
    "\n",
    "metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy\n",
    "    ]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    llm = get_gemini_pro(),\n",
    "    embeddings = hf,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ]\n",
    ")\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\evaluate\\gemini-1.5-pro_hybrid_@3_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_dict = {\n",
    "    \"model - retrieval_type\": [],\n",
    "    \"faithfulness\": [],\n",
    "    \"answer_relevancy\": [],\n",
    "    \"context_recall\": [],\n",
    "    \"context_precision\": []\n",
    "}\n",
    "\n",
    "path = r\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\evaluate\\*.csv\"\n",
    "\n",
    "for file_path in glob.glob(path):\n",
    "    file_name = os.path.basename(file_path) \n",
    "    df = pd.read_csv(file_path) \n",
    "\n",
    "    required_columns = [\"faithfulness\", \"answer_relevancy\", \"context_recall\", \"context_precision\"]\n",
    "    if all(col in df.columns for col in required_columns):\n",
    "        # Tính trung bình của từng cột\n",
    "        mean_faithfulness = df[\"faithfulness\"].mean()\n",
    "        mean_answer_relevancy = df[\"answer_relevancy\"].mean()\n",
    "        mean_context_recall = df[\"context_recall\"].mean()\n",
    "        mean_context_precision = df[\"context_precision\"].mean()\n",
    "\n",
    "        data_dict[\"model - retrieval_type\"].append(file_name)\n",
    "        data_dict[\"faithfulness\"].append(mean_faithfulness)\n",
    "        data_dict[\"answer_relevancy\"].append(mean_answer_relevancy)\n",
    "        data_dict[\"context_recall\"].append(mean_context_recall)\n",
    "        data_dict[\"context_precision\"].append(mean_context_precision)\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: File {file_name} is missing required columns!\")\n",
    "\n",
    "df_summary = pd.DataFrame(data_dict)\n",
    "\n",
    "df_summary.to_csv(\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\evaluate\\summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model - retrieval_type</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemini-1.5-flash_hybrid_@10_result.csv</td>\n",
       "      <td>0.813941</td>\n",
       "      <td>0.543173</td>\n",
       "      <td>0.520408</td>\n",
       "      <td>0.317026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-1.5-flash_hybrid_@1_result.csv</td>\n",
       "      <td>0.844411</td>\n",
       "      <td>0.431037</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.244898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-1.5-flash_hybrid_@3_result.csv</td>\n",
       "      <td>0.739067</td>\n",
       "      <td>0.505019</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.306973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-1.5-flash_hybrid_@5_result.csv</td>\n",
       "      <td>0.808912</td>\n",
       "      <td>0.545636</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.317347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemini-1.5-flash_parent_@10_result.csv</td>\n",
       "      <td>0.822197</td>\n",
       "      <td>0.460817</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.189776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini-1.5-flash_parent_@1_result.csv</td>\n",
       "      <td>0.694504</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.153061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemini-1.5-flash_parent_@3_result.csv</td>\n",
       "      <td>0.765098</td>\n",
       "      <td>0.431593</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.254252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemini-1.5-flash_parent_@5_result.csv</td>\n",
       "      <td>0.828747</td>\n",
       "      <td>0.474459</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.240079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemini-1.5-pro_hybrid_@10_result.csv</td>\n",
       "      <td>0.894313</td>\n",
       "      <td>0.662061</td>\n",
       "      <td>0.540816</td>\n",
       "      <td>0.354110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemini-1.5-pro_hybrid_@1_result.csv</td>\n",
       "      <td>0.852771</td>\n",
       "      <td>0.563584</td>\n",
       "      <td>0.251753</td>\n",
       "      <td>0.336735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini-1.5-pro_hybrid_@3_result.csv</td>\n",
       "      <td>0.820566</td>\n",
       "      <td>0.624639</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.331633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini-1.5-pro_hybrid_@5_result.csv</td>\n",
       "      <td>0.849556</td>\n",
       "      <td>0.621098</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.377721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gemini-1.5-pro_parent_@10_result.csv</td>\n",
       "      <td>0.787980</td>\n",
       "      <td>0.460863</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.212994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gemini-1.5-pro_parent_@1_result.csv</td>\n",
       "      <td>0.701865</td>\n",
       "      <td>0.398327</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.173469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemini-1.5-pro_parent_@3_result.csv</td>\n",
       "      <td>0.695548</td>\n",
       "      <td>0.405577</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemini-1.5-pro_parent_@5_result.csv</td>\n",
       "      <td>0.715387</td>\n",
       "      <td>0.418185</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.259864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model - retrieval_type  faithfulness  answer_relevancy  \\\n",
       "0   gemini-1.5-flash_hybrid_@10_result.csv      0.813941          0.543173   \n",
       "1    gemini-1.5-flash_hybrid_@1_result.csv      0.844411          0.431037   \n",
       "2    gemini-1.5-flash_hybrid_@3_result.csv      0.739067          0.505019   \n",
       "3    gemini-1.5-flash_hybrid_@5_result.csv      0.808912          0.545636   \n",
       "4   gemini-1.5-flash_parent_@10_result.csv      0.822197          0.460817   \n",
       "5    gemini-1.5-flash_parent_@1_result.csv      0.694504          0.303300   \n",
       "6    gemini-1.5-flash_parent_@3_result.csv      0.765098          0.431593   \n",
       "7    gemini-1.5-flash_parent_@5_result.csv      0.828747          0.474459   \n",
       "8     gemini-1.5-pro_hybrid_@10_result.csv      0.894313          0.662061   \n",
       "9      gemini-1.5-pro_hybrid_@1_result.csv      0.852771          0.563584   \n",
       "10     gemini-1.5-pro_hybrid_@3_result.csv      0.820566          0.624639   \n",
       "11     gemini-1.5-pro_hybrid_@5_result.csv      0.849556          0.621098   \n",
       "12    gemini-1.5-pro_parent_@10_result.csv      0.787980          0.460863   \n",
       "13     gemini-1.5-pro_parent_@1_result.csv      0.701865          0.398327   \n",
       "14     gemini-1.5-pro_parent_@3_result.csv      0.695548          0.405577   \n",
       "15     gemini-1.5-pro_parent_@5_result.csv      0.715387          0.418185   \n",
       "\n",
       "    context_recall  context_precision  \n",
       "0         0.520408           0.317026  \n",
       "1         0.234694           0.244898  \n",
       "2         0.346939           0.306973  \n",
       "3         0.459184           0.317347  \n",
       "4         0.459184           0.189776  \n",
       "5         0.112245           0.153061  \n",
       "6         0.326531           0.254252  \n",
       "7         0.408163           0.240079  \n",
       "8         0.540816           0.354110  \n",
       "9         0.251753           0.336735  \n",
       "10        0.448980           0.331633  \n",
       "11        0.489796           0.377721  \n",
       "12        0.479592           0.212994  \n",
       "13        0.173469           0.173469  \n",
       "14        0.306122           0.265306  \n",
       "15        0.377551           0.259864  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Documents\\TDTU\\ProjectIT\\Chatbot_RAG_2024\\data\\evaluate\\gemini-1.5-pro_hybrid_@10_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
