import time
import random
import streamlit as st
from seed_data import get_retriever
from agent import (legal_response, 
                   normal_response, 
                   fusion_ranker,
                   original_ranker, 
                   get_router,
                   get_gemini_llm,
                   get_encoder_model,
                   get_vertex_llm
                   )
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

def reset_conversation():
    """
    ƒê·∫∑t l·∫°i to√†n b·ªô tr·∫°ng th√°i c·ªßa cu·ªôc tr√≤ chuy·ªán.
    """
    st.session_state['messages'] = [] 

def response_generator(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.05)
    
def setup_page():
    """
    C·∫•u h√¨nh trang web c∆° b·∫£n
    """
    st.set_page_config(
        page_title="Legal Assistant", 
        page_icon="üí¨",
        layout="wide" 
    )

def setup_sidebar():
    with st.sidebar:
        st.title("‚öôÔ∏è **C·∫§U H√åNH**")
        st.header(":desktop_computer: Response Model")
        model_choice = st.radio(
            "Ch·ªçn Model ƒë·ªÉ tr·∫£ l·ªùi:",
            ["Vertex", "Gemini"]
        )
        
        st.button("Clear chat", on_click=reset_conversation)
        return model_choice
        
def setup_chat_interface(model_choice):
    if model_choice == "Gemini":
        st.title("Legal Assistant")
        st.caption("Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† Google")
    if model_choice == "Vertex":
        st.title("Legal Assistant")
        st.caption("Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi LangChain v√† Vertex AI")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    if "messages" not in st.session_state:
        st.session_state['messages'] = [
            {"role": "assistant", "content": random.choice(
                [
                    "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω c·ªßa b·∫°n. H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
                    "Ch√†o b·∫°n! B·∫°n c·∫ßn t√¥i gi√∫p g√¨ kh√¥ng? T√¥i c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô c·ªßa b·∫°n.",
                    "B·∫°n mu·ªën bi·∫øt g√¨ v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô? H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v√†o √¥ b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi t√¥i.",
            ])}
        ]
        msgs.add_ai_message(st.session_state.messages[0]["content"])
    
    for msg in st.session_state['messages']:
        with st.chat_message(msg["role"]):
            # response = st.write_stream(response_generator(msg["content"]))
            st.markdown(msg["content"])
    return msgs

def user_input(msgs, model_choice, encode_model, new_retriever, old_retriever):
    if prompt:= st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô!"):
        st.session_state['messages'].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)   
        msgs.add_user_message(prompt)
        router = get_router(prompt, model_choice)
        router, status = router.split(",")
        router = router.strip(" ")
        status = status.strip(" ")
        with st.chat_message("assistant"):
            with st.spinner("ƒêang t√¨m ki·∫øm th√¥ng tin..."):
            # st_callback = StreamlitCallbackHandler(st.container())
                chat_history = [
                    {"role": msg["role"], "content": msg["content"]}
                    for msg in st.session_state.messages[:-1]
                ]
                if router == "yes":
                    if status == "new":
                        context = original_ranker(prompt, encode_model, "new-documents-hybrid")
                        
                    elif status == "old":
                        context = original_ranker(prompt, encode_model, "old-documents-hybrid")
                    # context = original_ranker(prompt, encode_model, new_retriever)
                    
                    # context = fusion_ranker(prompt, new_retriever)
                    response = legal_response(msgs, model_choice, context, chat_history)
                    # else:
                    #     context = fusion_ranker(prompt, llm, old_retriever)
                    #     response = legal_response(prompt, llm, context, msgs)
                        
                elif router == "no":
                    response = normal_response(prompt, model_choice, chat_history)
                else:
                    response = "Kh√¥ng t√¨m th·∫•y th√¥ng tin cho n·ªôi dung b·∫°n t√¨m ki·∫øm!"
                    # response = prompt
                response = st.write_stream(response_generator(response))
                st.session_state['messages'].append({"role": "assistant", "content": response})
                msgs.add_ai_message(response)
                # st.markdown(response)
def main():
    setup_page()
    model_choice = setup_sidebar()
    # llm = get_vertex_llm()
    encode_model = get_encoder_model()
    new_retriever = get_retriever("new-documents-hybrid")
    old_retriever = get_retriever("old-documents-hybrid")
    msgs = setup_chat_interface(model_choice)
    user_input(msgs, model_choice, encode_model, new_retriever, old_retriever)
    
if __name__ == "__main__":
    main()